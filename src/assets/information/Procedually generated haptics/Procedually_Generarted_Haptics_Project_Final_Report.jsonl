{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 1, "text": "üßòüèæ‚Äç‚ôÇÔ∏è\nIndustry Project: Final Report\nAffective State Change via Procedurally \nGenerated Haptics\nüë¨by Vincent G√∂ke | Moritz Sendner | JM Santiago III\nüîäin collaboration with Daniel Shor from Innovobot Labs Inc.\nüîñResearch Library (Zotero): \nhttps://www.zotero.org/groups/5177784/industry_project/library\nüíªGitHub Project Page (MVP code): \nhttps://github.com/NesR0M/Industry_Project/tree/main\nIndustry Project: Final Report\n1"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 2, "text": "ü§ñLive Page (MVP): https://react-midi.netlify.app/\nProject Summary\nApplication Context\nFor our Industry Project, we explored various directions, including artificial \nintelligence (AI), to procedurally generate long-form audio, additionally focusing \non their musical patterns. This audio is intended for playback through a haptic \nactuator, aimed at inducing a state of relaxation leading to a state of flow. \nFollowing initial research and conceptualization, we developed a suitable \nprototype for producing this long-form audio. With this project, we are \ncontributing to the field of emotional well-being applications aimed at users who \nwant to improve their emotional state during concentrated productivity activities \nor for later relaxation.\nProblem Statement and Motivation\nIn our fast-paced modern world, many people struggle with stress, lack of \nconcentration, and emotional imbalance. This project addresses the need for a \nsolution that can conveniently and effectively help users achieve their desired \nemotional states, such as relaxation and flow during or before the start of a \nproductive work session. The motivation behind this project is to utilize generative \nAI and haptics to create an accessible emotional enhancement tool that meets the \nvarious needs of users in their daily lives and provides valuable insights into the \nfield of emotional state change technology development.\nGoal of the Project\nOur main goal was to investigate the emotional states of flow and relaxation, \ndesign a concept for inducing these states, and construct a basic proof-of-\nconcept prototype to evaluate our tool's effectiveness. Additionally, we aimed to \nexplore potential embodiments for this technology. This prototype aims to \nleverage AI-generated long-form audio, played through a haptic speaker, to \nIndustry Project: Final Report\n2"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 3, "text": "induce various affective states. The goal of this application is to improve users' \nemotional well-being, especially during study or work sessions, by offering \npersonalized and guided audio-based haptic experiences that promote states \nsuch as relaxation and flow.\nInitial Project Plan\nOur initial approach was to only focus on flow:\n1. Phase 1: Initial Research\na. Desk Research:\ni. Understand the concept of flow.\nii. Explore how AI can be utilized.\nb. Practical Research:\ni. Analyze existing music playlists and scientifically supported \napplications (like Endel) for soundscapes to identify essential \ncharacteristics for effective flow states.\nii. Investigate existing generative AIs for audio creation and their \nlimitations.\n2. Phase 2: Concept Creation\na. Based on the findings, create a concept (using only AI, partly AI and code, \nor no AI, only standard code).\nb. Design a haptic feedback system to deliver the customized waveforms to \nthe user.\nc. Explore how this can be achieved in non-obtrusive ways for long periods.\nd. Explore possible embodiments like a neck pillow with built-in haptics, and \nwhat actuator with what resonance is working the best for this purpose.\n3. Phase 3: Concept Testing\na. Conduct qualitative user testing to assess perceived effectiveness.\n4. Phase 4: Build MVP for our Industry Partner\nIndustry Project: Final Report\n3"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 4, "text": "a. Integrate the AI-generated audio and haptic feedback system into a simple \nMVP mobile app using a front-end framework with parameters that can be \nadjusted in real time.\nb. The app should allow users to select a desired emotional state and then \ngenerate a personalized audio composition with haptic feedback.\n5. Phase 5: Evaluation (If Time Permits)\na. Conduct a second round of qualitative user testing to determine the \neffectiveness of the prototype.\nb. A sample comparative study may involve looking into the participant‚Äôs \nproductivity with or without the app\nc. The study will also collect feedback from participants on the app's overall \nexperience.\nActual Project Plan\nThe project plan unfolded as follows:\n1. Phase 1: Initial Research\na. Desk Research:\ni. Research to understand the concept of flow.\nii. Explored how AI can be utilized.\nb. Practical Research:\ni. We analyzed existing music playlists and scientifically supported \napplications (like Endel) for soundscapes to identify essential \ncharacteristics for effective flow states.\nii. Investigated existing generative AIs for audio creation and their \nlimitations.\n2. Phase 2: Concept Creation\na. We developed a concept for a flow-inducing soundwave that used fast-\npaced rhythmic beats (similar to a techno song) to induce a flow-like \nrhythmic state.\nIndustry Project: Final Report\n4"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 5, "text": "3. Phase 3: Concept Testing\na. Based on our initial concept, we created a flow soundwave for testing with \nhaptic actuators provided by our industry partner, using a very limited \nPython script (no GUI, no AI, hardcoded, no adjustments possible), also \nprovided by our industry partner.\nb. Additionally, we tested different types of embodiments (oen neck and one \nback pillow and various placements of haptic actuators on the body).\nc. We conducted a qualitative user study with a simple breath-time-reducing \nrelaxation soundwave provided by our industry partner and our flow-\ninducing soundwave. The relaxation soundwave worked well, but the flow \nsoundwave was perceived as unpleasant. Multiple potential embodiment \npreferences were shared with us.\n4. Phase 4: Second Research Phase\na. Based on our user testing results, we conducted a second research phase \nand found that inducing flow alone is too difficult due to multiple internal \nand external triggers that we cannot control. Therefore, we \nreconceptualized our framework to use a well-received relaxation baseline \npattern to induce relaxation and create a state of control, making it easier \nto induce a flow state.\n5. Phase 5: New Concept Creation\na. For our new relaxation-based framework, we developed a concept that \nincluded two components:\ni. A ‚Äúbaseline‚Äù, which is a code-created amplitude-modulated sine tone \nthat gradually decreases in frequency and wavelength (tempo), \nresulting in a calming effect that gradually reduces the breathing rate \nfrom 11 to 6 breaths per minute.\nii. ‚ÄúSparkles,‚Äù which are AI-generated nuanced notes created based on \nthe baseline to add texture and variety to the soundscape, preventing \nit from becoming monotonous and enhancing its richness and interest.\n6. Phase 6: Build MVP for Our Industry Partner\nIndustry Project: Final Report\n5"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 6, "text": "a. We developed a simple web-based MVP using the front-end framework \nReact, with the potential for later adaptation to React Native for mobile app \ndevelopment. The ‚Äúbaseline‚Äù structure was generated using code, and the \n‚Äúsparkles‚Äù were generated using GPT-4. The soundwaves were generated \nusing Tone.js. Additionally, we built an RNBO instrument designed for \nplayback, which was not used in the React prototype anymore.\nProject Outcome\nDescription of the Deliverable\nInitial Concept and Evolution\nOur project began with the goal of developing a mobile application aimed at \ngeneral users in the work environment, focusing on improving emotional well-\nbeing through procedurally generated audio signals that are translated into haptic \nfeedback via audio-haptic actuators. At the beginning of our project, our industry \npartner Daniel Shor from Innovobot gave us the choice of exploring relaxation, \nflow, or sleep. As relaxation is already a very well-known area, our team decided \nto explore the flow state instead, a concept from Mih√°ly Cs√≠kszentmih√°lyi.\n*https://en.wikipedia.org/wiki/File:Challenge_vs_skill.svg\nIndustry Project: Final Report\n6"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 7, "text": "The conducted research during this phase was guided by our industry partner. \nAfter understanding the context and problem statement, we conducted our first \npreliminary pilot user tests with three different haptic actuators which also came \nfrom our industry partner. The goal of the user testing was to test out a created \nflow soundwave based on our research and to explore potential embodiments for \na haptic device that would interface with the application. The knowledge gained \nfrom these tests, combined with research into AI models for audio generation, led \nto a significant shift from developing an application that triggers a flow state to \none that triggers relaxation for a better flow state. For this purpose, we carried out \na second research phase, as outlined in our project plan above.\nFinal Deliverable: Web Application\nAside from focusing on a relaxation tool we also changed the target group of our \ntool, we decided on developing a more specialized, testing tool, catering not just \nto general users but specifically to haptic designers and audio professionals. This \nwas influenced by our research findings and early development insights. We built \na web application-based MVP that allows haptic designers to create procedurally \ngenerated audio-based haptics, composed and controlled by MIDI data, to make \nthe import and export of compositions more viable. This platform offers insight \ninto possible future implementations of generative AI to create haptic experiences.\nFor this purpose, we had to update our original research plan, to incorporate:\n1. Desk Research: Conducting thorough research, investigating the emotional \nstates of relaxation to flow from the psychological perspective, to gain further \ninsights on how these emotional states could be established more reliably.  \nAdditionally continuing research on current AI capabilities in audio and haptic \ngeneration, and studying the impact of these technologies on emotional \nstates. \n2. Prototype Development:\nTech Stack: Developing a React-based web application that incorporates \nMIDI creation and playback with Tone.js as an open-source version and in \nthe end of the project with an alternative approach of cycling74‚Äôs \nRNBO.json Max 8 instrument patches, that give options to create more \nIndustry Project: Final Report\n7"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 8, "text": "versatile instruments, used to play the MIDI, resulting in these audio-based \nhaptic experiences when played on the audio-haptic actuators.\nSound Composition: Creating the prototype with two key components, as \nsuggested by the industry partner: a baseline and sparkles.\nBaseline: Developing a modulated sine wave with amplitude \nmodulation to mimic a calming breathing pattern, trying to lower the \nbreath rate with a change of BPM and frequency pitch of the sine wave \naudio composition.\nSparkles: Integrating AI-generated notes using GPT-4 with few \nshotting to enhance the baseline and create a richer auditory \nexperience by adding suitable harmonies, based on scientific \nfrequency calculations and musical scale analysis.\n3. Deployment: Launching the web app for web server application usage, \nallowing users to experience and provide feedback on the audio-haptic \nIndustry Project: Final Report\n8"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 9, "text": "compositions.\nKey Features\n1. Procedural Generation Capabilities: Users can set specific characteristics \nand parameters to generate audio compositions. This is powered by a blend of \nAI algorithms and manual controls, providing both precision and creativity in \ndesign.\n2. Customizable Parameters: Haptic designers have the flexibility to manipulate \na range of parameters, including frequency, amplitude, and modulation \nsettings, allowing for the creation of diverse and nuanced haptic feedback for \ninstant testing.\n3. Real-Time Previews: The application provides real-time rendering, enabling \ndesigners to hear and feel the haptic output as they adjust settings, ensuring \nthe end product aligns with their creative vision.\n4. Integration of Tone.js and RNBO.js: By utilizing advanced web audio \ntechnologies, the application provides robust and high-quality audio output, \nwhich is crucial for the precise development of haptic feedback. The RNBO \ninstrument would be optimized for haptic actuators.\n5. AI-Enhanced Creativity: By using GPT-4 LLM to generate MIDI compositions \nvia a few shots to create the note structure, the application offers new and \ndynamic possibilities in haptic design.\nApplication Context and Usage\nOur web application MVP offers a prototyping platform for professionals to swiftly \ngenerate relaxing haptic sine waves, as well as to explore, innovate, and enhance \nAI prompts for 'sparkles' feedback. It has potential applications across various \nindustries, including gaming, therapy, and office environments, wherever there is \na need for inducing relaxation.\nDetailed Overview of Intermediary Results\nThe project commenced with an extensive phase of desk research, which \nrevolved around the latest advancements in AI, audio, and haptic technology. Our \ninitial idea revolved around establishing a guide for the concept of induced flow \nIndustry Project: Final Report\n9"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 10, "text": "state, a highly focused and immersive mental state, through the use of specifically \ndesigned audio and haptic feedback. This concept was grounded in the \nhypothesis that certain auditory and tactile stimuli could effectively influence the \nuser‚Äôs emotional and cognitive state, thereby facilitating easier entry into the flow \nstate. \nWe initially aimed to address both relaxation and flow states. However, it became \nevident that encompassing both affective states within the project's scope and \ntime constraints was overly ambitious. Therefore, we made a strategic decision to \nnarrow our focus solely on relaxation. This refinement allowed for a more in-depth \nand targeted approach, ensuring the quality and feasibility of the project within \nthe allocated timeframe.\nUnderstanding Flow State\nThe first step was to delve deeply into the characteristics necessary to induce a \nflow state. This involved:\n1. Research on Flow Inducers: The team conducted independent research, \nguided by the industry partner and haptic expert Daniel Shor, to understand \nthe specific elements like Beats Per Minute (BPM), Breaths Per Minute (BrPM), \nhaptic frequency sweet spots, audio-haptic actuator specifics, and other sonic \ncharacteristics that could potentially induce flow or at least potentially \ninfluence our cognitive processing. A big part of the flow state establishment \nis also the intrinsic motivation mindset of the user, which can have various \nvariables influencing once‚Äôs capabilities to engage into a flow state. \n2. Few-shotting an LLM: The next step involved experimenting with a generative \npre-trained transformer (GPT-4) model to generate usable music compositions \nin midi JSON form. This approach involved fine-tuning GPT's understanding of \nmusic theory. During these experiments, we encountered some limitations in \nthe length of the response. Therefore, we also investigated ocal Large \nLanguage Models (LLM) as a promising alternative. However, this model \nproved to be impractical in the context of our project. It gave good and long \nresponses but needed too much computing power to run as a simple \napplication on the web or as an app.\nExploration of Haptic Development\nIndustry Project: Final Report\n10"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 11, "text": "Parallel to the audio component, we also looked into developing haptic feedback \nmechanisms, which included:\n1. SuperCollider Program Template: A program could be designed in \nSuperCollider, an environment and programming language for real-time audio \nsynthesis, where variables could be manipulated to create specific waveforms \nthat would be translated into tactile feedback. However, we decided against \nusing SuperCollider due to its complexity.\n2. Integration with Interhaptics Haptic Composer: We also explored methods to \nintegrate these waveforms into Interhaptics Haptic Composer, an industry-\nknown tool that would allow the translation of audio signals into haptic \nfeedback. After a short test phase, we decided against this tool as the existing \napp was only available on Android devices and was unfortunately very buggy.\n3. Implementation on Haptic Devices: The final step in this segment was \nfiguring out how to run these haptics on a physical haptic device, turning the \ndigital signals into actual tactile experiences.\nThose approaches did not result in a fruitful manner, so we decided to first \napproach the audio-haptic actuators closer, to determine their characteristics and \nsuitability four our cause. \nExploration of Audio Generation\nAt the beginning, we investigated various generative AI models and services. This \nexploration aimed to identify models that could seamlessly integrate with our \nexisting Python script.\n1. Audiocraft + Gradio\nUtilized Meta‚Äôs open-source music and audio-generative AI models.\nIncorporated Gradio to run a web app, simplifying user interaction.\nFound limitations in the variety of available models (primarily demo \nmodels), constraining the breadth of audio generation.\n2. Magenta‚Äôs MelodyRNN\nExplored open-source melody-generative AI models using RNN-LSTM.\nIndustry Project: Final Report\n11"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 12, "text": "The model generates melodies in a step-by-step process, akin to a Large \nLanguage Model (LLM).\nAvailability limited to a Python library with no interactive playground or \ndemo for user testing.\n3. Stability AI‚Äôs Dance Diffusion\nExamined a collection of audio-generating machine learning models from \nHarmonAI under Stability AI.\nConsidered exploring more of HarmonAI‚Äôs work.\nAccess to the model was restricted to demos on Google Colab, presenting \nchallenges for direct integration.\n4. Non-AI Audio Creation\nAs an alternative, a template was planned in MAXSP, a visual programming \nlanguage for music and multimedia, to manually create audio waveforms.\nPreliminary User Testing\nAs mentioned previously, we also conducted preliminary user testing to determine \nthe effectiveness of our conducted desk research. For that, we tested two \nsoundwaves one from our industry partner for relaxation and one flow state-\ninducing wavelength along with various haptic actuators and their potential \nembodiments. To make sure our first study on flow states was going the right way, \nthis testing phase was very important. It helped us see what users liked and \nwhether different designs of our devices worked well at work. We made two types \nof haptic devices (for the neck and back) and tested them in three situations.\n1. Prototypes\nNeck Pillow: Integrated with two wired haptic motors.\nBack Pillow: Featured one wireless haptic motor.\n2. Testing Scenarios\nFirst Prototype without Music: Users experienced a 10-minute audio \nsession (low intensity) and then took a 1-hour break. They maintained a \ndiary for before and after the test.\nIndustry Project: Final Report\n12"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 13, "text": "First Prototype with Music: Similar to the first scenario, but with the \ninclusion of music.\nSecond Prototype without Music: A 10-minute audio session (high \nintensity) was followed by a 1-hour break, with diary entries before and \nafter the test.\nSecond Prototype with Music: Similar to the first scenario, but with the \ninclusion of music.\n3. Final Interview\nAfter completing the testing scenarios, users participated in a final \ninterview. This provided valuable insights into their experiences, \npreferences, and the overall impact of the haptic devices in conjunction \nwith the audio application.\nShift in Target Audience and Platform\nDuring the early stages of development, our team envisioned creating a mobile \napplication aimed at general users. This concept was driven by the desire to make \naffective state changes accessible to a broad audience. However, as the project \nevolved, we encountered several challenges and opportunities that prompted a \nshift in our approach.\n1. Identifying a Niche Need: Our research and early feedback highlighted a \nspecific need within the haptic design community ‚Äì a tool that allows for the \ncreation of procedurally generated audio-based haptics. This realization \nsteered our focus towards a more niche but impactful area.\n2. Recognizing the Individuality of Flow States: Flow state establishment is a \nmore complicated and not so easily inducible state. There are various factors \nlike goal establishment, and mood similarities that help transition into flow \neasier (e.g. relaxation ‚Üí flow = easier than stress ‚Üí flow) and distraction is a \nbig factor in taking someone out of their flow state. These findings led us to \nthe realization, that a mixed approach with an initial, guided relaxation phase \nto set goals for the work session and creating an emotional baseline could \npotentially ease the user‚Äôs engagement and transition into a productive flow \nstate. \nIndustry Project: Final Report\n13"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 14, "text": "3. Redefining the Deliverable: In light of our new direction, we transitioned from \ndeveloping a user-centered mobile application to creating a web application \ntailored for haptic designers. This shift was not only aligned with the identified \nniche need but also offered greater flexibility and potential for professional \nuse.\n4. Proof of Concept Goal: The change in our target audience and platform was \nalso influenced by the project's revised goal to serve as a proof of concept. \nGiven the time constraints and the complex nature of integrating AI with audio \nand haptic feedback, a web application for haptic designers presented a more \nviable and focused approach. It allowed us to demonstrate the core \nfunctionalities and potential of our concept, setting the foundation for future \nenhancements and expansions.\nWe then came up with an iterated idea for our project, adapting to new insights \nand practical considerations. While the original focus was on creating a tool to \ninduce flow state through audio-based haptic feedback, the revised concept \nshifted towards a more feasible and research-informed relaxation to ‚Äúcreate a \nbase for flow‚Äù approach. Here‚Äôs an overview of the key elements of the iterated \nidea:\nOur Guiding Star üåü\nThe guiding principle of the revised project was the assumption that accessing \nflow states is more achievable from a relaxed state. This approach was based on \nthe understanding that relaxation could serve as a preparatory phase, helping \nusers to set goals and structure their thoughts, thereby reducing anxiety. This \nstructured and calm mindset was seen as a potential foundation for more \neffectively inducing flow states.\nRevised Framework\nThe revised approach was structured around two main components: Baseline and \nSparkles.\nBaseline Audio\nUtilizing the Existing Python script: We planned to use the Python script \nprovided by our industry partner for creating the baseline audio. However, this \nwas later revised into Javascript using Tone.js as a base library for producing \nIndustry Project: Final Report\n14"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 15, "text": "the sounds. For the future, we imagined an alternative version and substituted \nthe Tone.js instrument with a more versatile RNBO web-audio export patch.\nUser Control Over Inputs: We planned that a future version of the application \nwould enable users to dynamically shift inputs according to an emotional grid, \ninfluencing the baseline audio to match the desired state and their complexity \npreferences to target the ideal engagement factor and have it as non-intrusive \nas possible.\nIntegration of the Max 8‚Äôs RNBO Object: By creating a javascript RNBO \ninstrument patch, we would be able to play the generated MIDI on different \ninstrument types to allow for rapid prototyping of waveforms and testing on \ndifferent actuators with adjustable instrumentalization.\nTwo Different Approaches: For the creation of baseline audio, two \napproaches were possible. The basic concept is to lower the notes played \nwhile simultaneously manipulating the tempo to have the last third of the \ngenerated audio on the target frequency for the Breaths Per Minute (BrPM), \nscientifically backed with relaxation.  \nLinear MIDI + Instrument Pitch: The first utilizes linear MIDI creation that \nplays an instrument which will be pitch shifted to have the gradual detune \neffect for the relaxation. For example, one can choose a starting frequency \nwhere an associated note will be chosen as the baseline note and \ndetermining the musical scale of the composition. This composition can \nthen be played linearly given the pitch of the instrument, playing the same \ntone and a melody suitable to that base tone) will be manipulated to \ngradually lower the compositions overall frequency. The tempo will be \nseparately slowed and has no effect on the pitch logic of this approach.\nLadder MIDI: The second utilizes the MIDI ladder concept, where fixed \nnotes are played. The notes are associated with a certain frequency pitch, \ne.g. concert pitch of A4=440Hz, and the change in the frequency \nparameter towards lower and slower notes will be nudged to the closest \nnote associated with the current value. This concludes in a MIDI \ncomposition, that when reaching a pivotal frequency threshold, the next \ngenerated note will be a lower one. \nDynamic Sparkles\nIndustry Project: Final Report\n15"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 16, "text": "AI-Generated Audio: The plan included using GPT-4 to dynamically generate \naudio that would layer over the baseline audio, aiding in inducing relaxation. \nThese ‚Äúsparkles‚Äù can be differentiated into Chords, Ladders and Twinkles:\nChords are Triads, Ladders are chords that are split and played up and down and \nTwinkles are short notes with no predefined sequence.\nIncorporating Natural Sound Characteristics: We considered integrating \nsounds found in nature (such as bird sounds or flowing water) to enrich the \naudio experience. This could later be implemented in future work on the \nprototype, considering technological advancements in AI Audio generation like \nthe audio sound effect generation model ‚ÄúAudiobox‚Äù by Meta.\nOutcomes of the Shift\nEnhanced Focus on Quality: By narrowing our scope to establish the first step \ninto relaxation and targeting haptic designers, we were able to concentrate our \nresources on developing an MVP.\nIncreased Technical Depth: By switching to a web application, we allow for a \nmore modular approach thanks in part to the framework we‚Äôre utilizing. By \nusing the React Javascript library as a base, we allow for the creation of \ncomponents that can be rearranged based on the needs of the professional. \nThis also allows for the use of an extensive list of Javascript libraries such as \nOpenAI, Tone.js, RNBO.js, Bootstrap, etc.\nFoundation for Future Expansion: The proof of concept established with this \nweb application lays a solid groundwork for future development. It provides a \nplatform that can be improved and potentially expanded to include additional \naffective states and different context use cases apart from relaxation \nenhancement and stress reduction in work settings.\nIndustry Project: Final Report\n16"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 17, "text": "Building the MVP\nAs previously mentioned, our MVP was a React-based prototype. We anticipated \nthat this would facilitate the transition to a mobile app or enable it to function as a \nweb application if required later. Additionally, this approach allowed us to utilize \nsound libraries like Tone.js and paved the way for future integration of a digital \ninstrument (specifically designed for haptics) created using software called RNBO. \nThe design of the Baseline and Sparkles as separate code components makes it \npossible to easily swap these elements, further enhancing the application's \nversatility.\nThe baseline was created only through code on parameters like frequency given \nby the user.\nThe up and down modulated sine wave effect was created through attack and \nrelease filters on single midi notes:\nIndustry Project: Final Report\n17"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 18, "text": "For the ‚Äúsparkles‚Äù, we created prompts to teach GPT-4 how to compose musical \nnotes in the way we wanted them (example for the twinkles prompt):\nexport const promptTwinkles = \n`I need assistance in producing AI-generated text that I convert to music using M-\n1. **Note Specification**: Use pitch values corresponding to frequencies in the 2-\n2. **Integration of Silence**: Include silences, denoted by a tuple with pitch 0 an\nIndustry Project: Final Report\n18"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 19, "text": "3. **Musical Elements**:\n1. **Preferred Frequencies**: 40-60 Hz are a pleasant range for tactile vibrations-\n2. **Consonance and Dissonance**: The papers extend the concept of consonan\n3. **Individual Variability**: While certain frequencies and intervals may be broad\n4. **Rhythmic and Smooth Patterns**: Preferred are non intrusive, rhythmic and \n5. **Resonance and Neural Entrainment**: The concept of resonance, particularl\n- **Pitch Range**: Emphasize pitches that align with or build on frequencies foun\n    - **Rhythm and Contour**: Develop rhythms and contours reflecting preferenc\n    - **Consonance and Dissonance**: Apply principles of consonance and disso(:)(:):\nSyntax:\n{\n// the transport and timing data\nheader: {\nname: String,                     // the name of the first empty track, \n                                  // which is usually the song name\ntempos: TempoEvent[],             // the tempo, e.g. 120\ntimeSignatures: TimeSignatureEvent[],  // the time signature, e.g. [4, 4],\nPPQ: Number                       // the Pulses Per Quarter of the midi file\n                                  // this is read only\n},\nduration: Number,                   // the time until the last note finishes\n// an array of midi tracks\ntracks: [\n{\n  name: String,                   // the track name if one was given\n  channel: Number,                // channel\n                                  // the ID for this channel; 9 and 10 are\n                                  // reserved for percussion\n  notes: [\n    {\n      midi: Number,               // midi number, e.g. 60\n      time: Number,               // time in seconds\nIndustry Project: Final Report\n19"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 20, "text": "ticks: Number,              // time in ticks\n      name: String,               // note name, e.g. \"C4\",\n      pitch: String,              // the pitch class, e.g. \"C\",\n      octave : Number,            // the octave, e.g. 4\n      velocity: Number,           // normalized 0-1 velocity\n      duration: Number,           // duration in seconds between noteOn and noteOff\n    }\n  ],\n  // midi control changes\n  controlChanges: {\n    // if there are control changes in the midi file\n    '91': [\n      {\n        number: Number,           // the cc number\n        ticks: Number,            // time in ticks\n        time: Number,             // time in seconds\n        value: Number,            // normalized 0-1\n      }\n    ],\n  },\n  instrument: {                   // and object representing the program change events\n    number : Number,              // the instrument number 0-127\n    family: String,               // the family of instruments, read only.\n    name : String,                // the name of the instrument\n    percussion: Boolean,          // if the instrument is a percussion instrument\n  },          \n}\n]\n} here is a filled out example: \"{\n\"header\": {\n\"keySignatures\": [],\n\"meta\": [],\n\"name\": \"\",\n\"ppq\": 240,\nIndustry Project: Final Report\n20"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 21, "text": "\"tempos\": [\n  {\n    \"bpm\": 120,\n    \"ticks\": 0\n  }\n],\n\"timeSignatures\": [\n  {\n    \"ticks\": 0,\n    \"timeSignature\": [\n      4,\n      4\n    ],\n    \"measures\": 0\n  }\n]\n},\n\"tracks\": [\n{\n  \"channel\": 0,\n  \"controlChanges\": {\n    \"7\": [\n      {\n        \"number\": 7,\n        \"ticks\": 0,\n        \"time\": 0,\n        \"value\": 1\n      }\n    ]\n  },\n  \"pitchBends\": [],\n  \"instrument\": {\n    \"family\": \"bass\",\n    \"number\": 33,\n    \"name\": \"electric bass (finger)\"\n  },\nIndustry Project: Final Report\n21"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 22, "text": "\"name\": \"\",\n  \"notes\": [\n    {\n      \"duration\": 0.26875,\n      \"durationTicks\": 129,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 0,\n      \"time\": 0,\n      \"velocity\": 0.6141732283464567\n    },\n    {\n      \"duration\": 0.2562500000000001,\n      \"durationTicks\": 123,\n      \"midi\": 48,\n      \"name\": \"C3\",\n      \"ticks\": 360,\n      \"time\": 0.75,\n      \"velocity\": 0.6377952755905512\n    },\n    {\n      \"duration\": 0.26875000000000004,\n      \"durationTicks\": 129,\n      \"midi\": 50,\n      \"name\": \"D3\",\n      \"ticks\": 480,\n      \"time\": 1,\n      \"velocity\": 0.6220472440944882\n    },\n    {\n      \"duration\": 0.1333333333333333,\n      \"durationTicks\": 64,\n      \"midi\": 52,\n      \"name\": \"E3\",\n      \"ticks\": 600,\n      \"time\": 1.25,\nIndustry Project: Final Report\n22"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 23, "text": "\"velocity\": 0.5354330708661418\n    },\n    {\n      \"duration\": 0.7333333333333334,\n      \"durationTicks\": 352,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 840,\n      \"time\": 1.75,\n      \"velocity\": 0.6456692913385826\n    },\n    {\n      \"duration\": 0.4791666666666665,\n      \"durationTicks\": 230,\n      \"midi\": 43,\n      \"name\": \"G2\",\n      \"ticks\": 1200,\n      \"time\": 2.5,\n      \"velocity\": 0.4881889763779528\n    },\n    {\n      \"duration\": 0.28125,\n      \"durationTicks\": 135,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 1440,\n      \"time\": 3,\n      \"velocity\": 0.6299212598425197\n    },\n    {\n      \"duration\": 0.16041666666666643,\n      \"durationTicks\": 77,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 1920,\n      \"time\": 4,\nIndustry Project: Final Report\n23"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 24, "text": "\"velocity\": 0.6929133858267716\n    },\n    {\n      \"duration\": 0.16041666666666643,\n      \"durationTicks\": 77,\n      \"midi\": 48,\n      \"name\": \"C3\",\n      \"ticks\": 2280,\n      \"time\": 4.75,\n      \"velocity\": 0.6062992125984252\n    },\n    {\n      \"duration\": 0.25416666666666643,\n      \"durationTicks\": 122,\n      \"midi\": 50,\n      \"name\": \"D3\",\n      \"ticks\": 2400,\n      \"time\": 5,\n      \"velocity\": 0.7086614173228346\n    },\n    {\n      \"duration\": 0.2020833333333334,\n      \"durationTicks\": 97,\n      \"midi\": 52,\n      \"name\": \"E3\",\n      \"ticks\": 2520,\n      \"time\": 5.25,\n      \"velocity\": 0.5905511811023622\n    },\n    {\n      \"duration\": 0.760416666666667,\n      \"durationTicks\": 365,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 2760,\n      \"time\": 5.75,\nIndustry Project: Final Report\n24"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 25, "text": "\"velocity\": 0.5905511811023622\n    },\n    {\n      \"duration\": 0.47916666666666696,\n      \"durationTicks\": 230,\n      \"midi\": 48,\n      \"name\": \"C3\",\n      \"ticks\": 3120,\n      \"time\": 6.5,\n      \"velocity\": 0.6062992125984252\n    },\n    {\n        \"duration\": 0.1875,\n        \"durationTicks\": 90,\n        \"midi\": 45,\n        \"name\": \"A2\",\n        \"ticks\": 3360,\n        \"time\": 7,\n        \"velocity\": 0.6141732283464567\n      }\n  ],\n  \"endOfTrackTicks\": 3839\n}\n]\n};\"\nHere is another example: {\n\"header\": {\n\"keySignatures\": [],\n\"meta\": [],\n\"name\": \"\",\n\"ppq\": 240,\n\"tempos\": [\n  {\n    \"bpm\": 120,\n    \"ticks\": 0\nIndustry Project: Final Report\n25"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 26, "text": "}\n],\n\"timeSignatures\": [\n  {\n    \"ticks\": 0,\n    \"timeSignature\": [\n      4,\n      4\n    ],\n    \"measures\": 0\n  }\n]\n},\n\"tracks\": [\n{\n  \"channel\": 0,\n  \"controlChanges\": {\n    \"7\": [\n      {\n        \"number\": 7,\n        \"ticks\": 0,\n        \"time\": 0,\n        \"value\": 1\n      }\n    ]\n  },\n  \"pitchBends\": [],\n  \"instrument\": {\n    \"family\": \"bass\",\n    \"number\": 33,\n    \"name\": \"electric bass (finger)\"\n  },\n  \"name\": \"\",\n  \"notes\": [\n    {\n      \"duration\": 0.26875,\nIndustry Project: Final Report\n26"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 27, "text": "\"durationTicks\": 129,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 0,\n      \"time\": 0,\n      \"velocity\": 0.6141732283464567\n    },\n    {\n      \"duration\": 0.2562500000000001,\n      \"durationTicks\": 123,\n      \"midi\": 48,\n      \"name\": \"C3\",\n      \"ticks\": 360,\n      \"time\": 0.75,\n      \"velocity\": 0.6377952755905512\n    },\n    {\n      \"duration\": 0.26875000000000004,\n      \"durationTicks\": 129,\n      \"midi\": 50,\n      \"name\": \"D3\",\n      \"ticks\": 480,\n      \"time\": 1,\n      \"velocity\": 0.6220472440944882\n    },\n    {\n      \"duration\": 0.1333333333333333,\n      \"durationTicks\": 64,\n      \"midi\": 52,\n      \"name\": \"E3\",\n      \"ticks\": 600,\n      \"time\": 1.25,\n      \"velocity\": 0.5354330708661418\n    },\n    {\n      \"duration\": 0.7333333333333334,\nIndustry Project: Final Report\n27"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 28, "text": "\"durationTicks\": 352,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 840,\n      \"time\": 1.75,\n      \"velocity\": 0.6456692913385826\n    },\n    {\n      \"duration\": 0.4791666666666665,\n      \"durationTicks\": 230,\n      \"midi\": 43,\n      \"name\": \"G2\",\n      \"ticks\": 1200,\n      \"time\": 2.5,\n      \"velocity\": 0.4881889763779528\n    },\n    {\n      \"duration\": 0.28125,\n      \"durationTicks\": 135,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 1440,\n      \"time\": 3,\n      \"velocity\": 0.6299212598425197\n    },\n    {\n      \"duration\": 0.16041666666666643,\n      \"durationTicks\": 77,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 1920,\n      \"time\": 4,\n      \"velocity\": 0.6929133858267716\n    },\n    {\n      \"duration\": 0.16041666666666643,\nIndustry Project: Final Report\n28"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 29, "text": "\"durationTicks\": 77,\n      \"midi\": 48,\n      \"name\": \"C3\",\n      \"ticks\": 2280,\n      \"time\": 4.75,\n      \"velocity\": 0.6062992125984252\n    },\n    {\n      \"duration\": 0.25416666666666643,\n      \"durationTicks\": 122,\n      \"midi\": 50,\n      \"name\": \"D3\",\n      \"ticks\": 2400,\n      \"time\": 5,\n      \"velocity\": 0.7086614173228346\n    },\n    {\n      \"duration\": 0.2020833333333334,\n      \"durationTicks\": 97,\n      \"midi\": 52,\n      \"name\": \"E3\",\n      \"ticks\": 2520,\n      \"time\": 5.25,\n      \"velocity\": 0.5905511811023622\n    },\n    {\n      \"duration\": 0.760416666666667,\n      \"durationTicks\": 365,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 2760,\n      \"time\": 5.75,\n      \"velocity\": 0.5905511811023622\n    },\n    {\n      \"duration\": 0.47916666666666696,\nIndustry Project: Final Report\n29"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 30, "text": "\"durationTicks\": 230,\n      \"midi\": 48,\n      \"name\": \"C3\",\n      \"ticks\": 3120,\n      \"time\": 6.5,\n      \"velocity\": 0.6062992125984252\n    },\n    {\n      \"duration\": 0.1875,\n      \"durationTicks\": 90,\n      \"midi\": 45,\n      \"name\": \"A2\",\n      \"ticks\": 3360,\n      \"time\": 7,\n      \"velocity\": 0.6141732283464567\n    }\n  ],\n  \"endOfTrackTicks\": 3839\n}\n]\n}\n5. **Individual Variability**: Recognize the variation in individual preferences for \nThe goal is to create melodies that are not only musically coherent but also optim\nThe answer was a list of notes in a Tone.js readable MIDI JSON format that were \nconverted into playable MIDI again.\nAnalysis of Project Outcome\nIndustry Project: Final Report\n30"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 31, "text": "The final deliverable of our project is the web application designed for haptic \ndesigners. \n1. AI-Driven Procedural Audio Generation: One of the application‚Äôs core \nexplorations was to generate unique audio compositions with LLM AI \ncapabilities. By few-shotting the GPT model with a JSON structure, which was \nparsable by our code, we were able to create playable MIDI files with a LLM. \nThis potentially offers a fresh perspective for establishing an AI promptable \nplayground for haptic designers. Our experiment aimed at the creation of \nharmonic soundscapes that possibly could add to the relaxation-inducing \nbaseline and therefore enhance the personal relaxation experience.\nIndustry Project: Final Report\n31"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 32, "text": "2. Customizable Haptic Feedback Parameters: Designers have extensive \ncontrol over haptic feedback parameters like frequency, tempo, and duration, \nessential for crafting diverse and nuanced haptic experiences suited to \nrelaxation scenarios.\n3. Responsive User Interface: The interface is intuitively designed, allowing easy \nnavigation, parameter adjustments, and previews of haptic designs. The \napplication is also responsive across different devices, ensuring flexibility and \naccessibility for designers using various platforms.\n4. Basic Sound Editing Tools: The application includes tools for basic audio \nediting, such as amplitude modulation and frequency adjustments, and allows \noverlaying multiple soundtracks for rich sound design that could translate into \ninteresting haptics.\n5. Integration with Tone.js and RNBO.js: Utilizing these advanced libraries, the \napp provides robust and high-fidelity audio capabilities, crucial for \nprofessional audio-based haptic design.\n6. Custom MIDI Files:\nUpload Capability: One of the application's key features is the ability for \nusers to upload their custom MIDI files for both the sparkles and baseline \ncomponents. This functionality greatly enhances the customization \npotential, enabling designers to bring their unique creative visions into \ntheir haptic designs.\nIndustry Project: Final Report\n32"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 33, "text": "Download Option: Additionally, the application provides the capability to \ndownload the final MIDI file. This feature is particularly valuable as it \nallows for external customization and further manipulation of the audio \ncomposition. Designers can use this downloaded file in other software or \nplatforms, offering even more flexibility and creative control in their design \nprocess.\n7. FX Features:\nAdding these additional audio effects can significantly alter the final haptic \nexperience.\nHigh-Pass and Low-Pass Filters: These filters enable designers to fine-\ntune their audio by controlling the frequency range that is let pass the \nfilters. This is crucial for crafting the desired auditory texture and higher the \nquality of the auditory signal from the perspective of audio engineering, \nfreeing up headroom for potentially more detailed and desirable haptic \nsensations.\nReverb Effects: Adds depth and space to the audio, allowing designers to \nsimulate different environmental acoustics for a more immersive haptic \nexperience.\nDelay Effects: Adds little pulses that mirror the first ‚Äúbig impact‚Äù of played \nhaptic audio. This can contribute to more complex haptic sensations while \nrequiring less compositional effort. \n1. Responsive Design for Various Devices: The react application is responsive \nacross different devices, ensuring flexibility and accessibility for designers \nusing various platforms. The core thought was to be able to connect a \nBluetooth device and just play the audio generated by the website through the \nconnected Bluetooth speaker, functioning as an audio-haptic actuator. \nThe project outcome, though divergent from the initial concept, stands as a \ntestament to our team's ability to navigate complex technological challenges and \ndeliver a product that significantly advances the field of experimental haptic \ndesign and AI-audio integration.\nComparison of Initial Goal vs. Final Outcome\nInitial Goal\nIndustry Project: Final Report\n33"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 34, "text": "The original aim of the project was to develop a mobile app that could induce a \nstate of flow through the integration of live AI-generated long-form audio \ntranslated into haptic feedback when played on a suitable audio-haptic actuator. \nThis concept was grounded in the belief that certain auditory and tactile stimuli, \nwhen orchestrated effectively, could significantly influence a user's cognitive and \nemotional state, facilitating the achievement of a highly focused and immersive \nflow state. Key elements of this initial goal included:\n1. Research on Flow State Inducers: In-depth exploration of the audio \ncharacteristics necessary to induce flow, such as BPM, frequency, and others.\n2. AI Model Training: Utilizing and fine-tuning GPT models to generate suitable \naudio content.\n3. Haptic Feedback Development: Creating a SuperCollider program template \nfor waveform generation and exploring the integration of these waveforms into \nhaptic devices.\n4. Optional Audio Development: Considering both AI-driven and non-AI \nmethods for additional audio generation, with tools like Audiocraft and MAXSP.\n5. Focus on flow State Characteristics: Emphasizing various audio elements like \nfrequency, amplitude, and rhythm to induce flow state.\nFinal Outcome\nThe project's final outcome, however, shifted significantly from this initial goal. \nThe revised project, while still rooted in the use of audio-haptic feedback, leans \nnow towards a more practical and achievable objective. The key changes in the \nfinal outcome included:\n1. Relaxation instead of Flow: The project shifted focus towards the idea of \nusing relaxation as a precursor to achieving a flow state. This concept was \nbased on the assumption that a relaxed state could set the stage for a more \neffective transition into flow.\n2. Infrastructure Development: Rather than completing the entire tool within the \nproject's timeframe, the team focused on building the foundational system that \ncould facilitate the future realization of the concept.\n3. Baseline and Dynamic Audio Components: The project emphasized \ndeveloping a system that balanced a hardcoded, yet individually adjustable, \nIndustry Project: Final Report\n34"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 35, "text": "stable baseline haptic audio and conducting the first experiments for enriching \nthe experience with subtle variations of dynamic, AI-generated melodies and \nharmonies.\n4. Practical Adjustments: Given the scope and resource limitations, the project \nstrategically adapted its goals, concentrating on creating an insightful MVP \nthat is a scalable and adaptable system rather than a fully realized product.\nAnalysis of the Shift\nThe shift from the initial goal to the outcome can be attributed to several factors:\nResource and Time Constraints: The ambitious nature of the original goal, \ncoupled with the time and resource limitations, necessitated a more pragmatic \napproach.\nTechnical Feasibility: As the project progressed, the technical challenges of \nintegrating AI-driven audio with haptic feedback became more apparent, \nleading to a refocus on building a foundational system.\nExpert Consultation and Market Viability: Feedback from experts and market \nanalysis highlighted the need for a more focused and validated approach, \nleading to our flow-through relaxation concept.\nAdaptability to Insights: The team's willingness to adapt to new insights and \npractical considerations allowed for a more achievable and strategically sound \noutcome, with ultimately interesting learnings, research insights and outlooks \nfor future projects.\nMain Roles and Work Distribution \nThe project's success was underpinned by the distinct yet interconnected roles of \neach team member. Their diverse expertise contributed to the multifaceted nature \nof the project, ensuring thorough research, innovative design, and effective \nimplementation. Below is a detailed overview of each member's roles and \ncontributions.\n1. Vincent G√∂ke\nRole: Sound Designer & Research Lead\nContributions:\nIndustry Project: Final Report\n35"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 36, "text": "Conducted extensive research on affective audio experiences and \ntheir potential translation into haptics, current research underlining \nhaptics' role in affective state change, with a focus on relaxation and \nflow states and psychological background for understanding the \nscientific state of research for these two states.\nLed the initial user testing for haptic embodiments, specifically \nintegrating a haptic speaker in a neck pillow. This was initialized to also \ntest the three different audio-haptic actuators that Daniel Shor gave us \nas tangible testing hardware. \nProvided expert consultation on all audio-related matters, ensuring the \naudio output was in line with project goals. Additionally proposed the \nMIDI framework to provide the best possible flexibility for potential \nreadjustments of technical implementations.\nEngaged in musical and haptic prompt engineering for GPT-4, tailoring \nit to meet specific conditions required for AI MIDI composition creation \nof the project.\nDesigned the RNBO.js instrumentalization solution for tone generation \nand gradual detuning, extending the application‚Äôs audio generation \ncapabilities.\n2. JM Santiago III\nRole: React Developer\nContributions:\nConducted research on generative AI solutions for audio and haptic \ngeneration.\nParticipated in the initial user testing phase, focusing on the integration \nof a haptic speaker in a neck pillow.\nLed the design and development of the React-based web application, \nensuring a user-friendly and functional interface.\nManaged the deployment of the final web application, overseeing its \nlaunch and functionality.\nIndustry Project: Final Report\n36"}
{"doc_id": "Procedually_Generarted_Haptics_Project_Final_Report.pdf", "page": 37, "text": "Responsible for documenting the entire project process, including \neach milestone and report, ensuring a comprehensive record of the \nproject‚Äôs evolution.\n3. Moritz Sendner\nRole: Framework Designer\nContributions:\nFocused on researching flow-state awareness and the influence of \nhaptics on this state.\nDesigned the final framework of the project that combines the initial \nbaseline sound with the dynamic sparkles sound, the basis of the web \napplication.\nPrepared and tested various materials for haptic embodiments while \naddressing all issues related to the haptic speakers, enhancing their \nperformance and reliability.\nEnhanced the initial Python script for baseline audio generation, \ncontributing to the foundational elements of the application.\nVery active developer in the reacted-based solution using Tone.js for \nthe baseline generation\nIndustry Project: Final Report\n37"}
